{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff7eb6-ffdd-4065-b47f-713270df2201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cee702-eb4d-4b88-b9c7-8f2afedc9d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\surendra.j.kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186e136-3ec4-4248-b44f-914ccb6fb12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c548ffe-200e-44de-b55d-dd173de2a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                  Basic_Mainframe_Access       1.00      1.00      1.00        15\n",
      "                           Clean_Up_RACF       1.00      1.00      1.00        19\n",
      "                               GuestWifi       1.00      1.00      1.00       109\n",
      "      Kiteworks_Add_User_Existing_Folder       1.00      1.00      1.00        63\n",
      "      Kiteworks_Create_New_Secure_Folder       1.00      1.00      1.00        66\n",
      "          Kiteworks_Remove_Secure_Folder       1.00      1.00      1.00        46\n",
      "            Off_Boarding_Integrity_Check       1.00      1.00      1.00        15\n",
      "                                  Others       1.00      1.00      1.00      2654\n",
      "                 Remove_Mainframe_Access       1.00      1.00      1.00        23\n",
      "                computer_integrity_check       1.00      1.00      1.00        25\n",
      "                   corporate_access_d365       1.00      1.00      1.00        52\n",
      "                      cross_boarding_imc       1.00      1.00      1.00        28\n",
      "                           intune_device       1.00      1.00      1.00        19\n",
      "          kiteworks_modify_secure_folder       1.00      1.00      1.00        16\n",
      "kiteworks_remove_user_from_secure_folder       1.00      1.00      1.00        36\n",
      "              review_offboarding_request       1.00      1.00      1.00        75\n",
      "\n",
      "                                accuracy                           1.00      3261\n",
      "                               macro avg       1.00      1.00      1.00      3261\n",
      "                            weighted avg       1.00      1.00      1.00      3261\n",
      "\n",
      "Predicted KBID: Others\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'sctask_data_dev_v13_utf8.csv'  # Update with your file path\n",
    "data = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Encode 'cat_item' as a numerical feature\n",
    "cat_item_encoder = LabelEncoder()\n",
    "data['cat_item_encoded'] = cat_item_encoder.fit_transform(data['cat_item'])\n",
    "\n",
    "# Load Hugging Face tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the short descriptions\n",
    "def tokenize_with_hf(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "data['short_description_tokenized'] = data['short_description'].apply(tokenize_with_hf)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=data['short_description_tokenized'], \n",
    "    vector_size=100,  # Size of word vectors\n",
    "    window=5,         # Context window size\n",
    "    min_count=1,      # Minimum word frequency\n",
    "    workers=4,        # Number of CPU threads\n",
    "    sg=1              # Skip-gram model\n",
    ")\n",
    "\n",
    "# Generate sentence embeddings by averaging word vectors\n",
    "def vectorize_sentence(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "data['short_description_vector'] = data['short_description_tokenized'].apply(\n",
    "    lambda tokens: vectorize_sentence(tokens, w2v_model, vector_size=100)\n",
    ")\n",
    "\n",
    "# Create the feature matrix\n",
    "X = pd.DataFrame(data['short_description_vector'].tolist())  # Expand vectors into separate columns\n",
    "X['cat_item_encoded'] = data['cat_item_encoded']\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Encode the target 'kbid'\n",
    "kbid_encoder = LabelEncoder()\n",
    "data['kbid_encoded'] = kbid_encoder.fit_transform(data['kbid'])\n",
    "y = data['kbid_encoded']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=kbid_encoder.classes_))\n",
    "\n",
    "# Save the encoders, model, and Word2Vec\n",
    "joblib.dump(model, 'kbid_predictor_model.pkl')\n",
    "joblib.dump(cat_item_encoder, 'cat_item_encoder.pkl')\n",
    "joblib.dump(kbid_encoder, 'kbid_encoder.pkl')\n",
    "w2v_model.save('word2vec_model.bin')\n",
    "\n",
    "# Usage example\n",
    "def predict_kbid(cat_item, short_description):\n",
    "    # Encode the categorical input\n",
    "    cat_item_encoded = cat_item_encoder.transform([cat_item])[0]\n",
    "    \n",
    "    # Tokenize and vectorize the short description\n",
    "    tokens = tokenizer.tokenize(short_description)\n",
    "    short_desc_vectorized = vectorize_sentence(tokens, w2v_model, vector_size=100)\n",
    "    \n",
    "    # Combine features\n",
    "    input_features = pd.DataFrame([short_desc_vectorized.tolist()])\n",
    "    input_features['cat_item_encoded'] = cat_item_encoded\n",
    "    \n",
    "    # Align columns with training data\n",
    "    input_features = input_features.reindex(columns=X.columns, fill_value=0)\n",
    "    \n",
    "    # Predict and decode the KBID\n",
    "    kbid_encoded = model.predict(input_features)[0]\n",
    "    return kbid_encoder.inverse_transform([kbid_encoded])[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2da6f-7612-4723-93cc-37bcbdcfcad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "example_prediction = predict_kbid(\"Asset Management\", \"Dax Studio - Power BI Software Tool Integration\")\n",
    "print(\"Predicted KBID:\", example_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305cefa-b0a4-4748-88f8-7c802f9137be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95025c22-4170-4511-963f-f633d5ffc861",
   "metadata": {},
   "outputs": [],
   "source": [
    "To use the saved model for making predictions on new data, you need to:\n",
    "\n",
    "Load the saved model and encoders.\n",
    "Load the Word2Vec model.\n",
    "Prepare the new input data (e.g., tokenize the short_description and encode cat_item).\n",
    "Align input features with the training data.\n",
    "Predict and decode the result.\n",
    "Here’s a step-by-step guide:\n",
    "\n",
    "Load the Saved Components\n",
    "First, ensure you’ve saved the following:\n",
    "\n",
    "Random Forest Model: kbid_predictor_model.pkl\n",
    "Category Encoder: cat_item_encoder.pkl\n",
    "KBID Encoder: kbid_encoder.pkl\n",
    "Word2Vec Model: word2vec_model.bin\n",
    "Now, load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83116e3-ad0b-4671-99fb-23a0bd9c7925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7c3a2-76c4-4a31-9a2c-c0aa414efb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved models and encoders\n",
    "model = joblib.load('kbid_predictor_model.pkl')\n",
    "cat_item_encoder = joblib.load('cat_item_encoder.pkl')\n",
    "kbid_encoder = joblib.load('kbid_encoder.pkl')\n",
    "w2v_model = Word2Vec.load('word2vec_model.bin')\n",
    "\n",
    "# Load Hugging Face tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bc667-d466-403b-8b39-554c49ec35fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36216247-d861-4adc-9c98-c13e6ce4ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prediction Function\n",
    "# The prediction function prepares new input data and uses the loaded model to predict.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "def vectorize_sentence(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "def predict_kbid(cat_item, short_description):\n",
    "    # Encode the categorical input\n",
    "    cat_item_encoded = cat_item_encoder.transform([cat_item])[0]\n",
    "    \n",
    "    # Tokenize and vectorize the short description\n",
    "    tokens = tokenizer.tokenize(short_description)\n",
    "    short_desc_vectorized = vectorize_sentence(tokens, w2v_model, vector_size=100)\n",
    "    \n",
    "    # Combine features\n",
    "    input_features = pd.DataFrame([short_desc_vectorized.tolist()])\n",
    "    input_features['cat_item_encoded'] = cat_item_encoded\n",
    "    \n",
    "    # Align columns with training data\n",
    "    input_features.columns = input_features.columns.astype(str)  # Ensure all column names are strings\n",
    "    input_features = input_features.reindex(columns=X_train.columns, fill_value=0)  # Replace X_train with your training data columns\n",
    "    \n",
    "    # Predict and decode the KBID\n",
    "    kbid_encoded = model.predict(input_features)[0]\n",
    "    return kbid_encoder.inverse_transform([kbid_encoded])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91179a-7d98-48ef-9a40-764681ba3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example Usage\n",
    "Here’s how to use the predict_kbid function for a new prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd668e5-985d-4145-b5c1-a5e3e44ec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input\n",
    "cat_item = \"Asset Management\"\n",
    "short_description = \"Dax Studio - Power BI Software Tool Integration\"\n",
    "\n",
    "# Predict KBID\n",
    "predicted_kbid = predict_kbid(cat_item, short_description)\n",
    "print(\"Predicted KBID:\", predicted_kbid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29554676-6adc-4f53-b066-9d3c919922b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3187db-4a11-4066-9a37-aa1655bb319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "What Happens in the Code\n",
    "Categorical Encoding:\n",
    "\n",
    "Encodes the cat_item using the cat_item_encoder.\n",
    "Text Tokenization:\n",
    "\n",
    "Tokenizes the short_description using Hugging Face’s tokenizer.\n",
    "Word Embedding:\n",
    "\n",
    "Generates the embedding by averaging Word2Vec vectors for the tokens.\n",
    "Feature Alignment:\n",
    "\n",
    "Aligns the features with the model's expected structure (e.g., matching column names).\n",
    "Prediction:\n",
    "\n",
    "Uses the Random Forest model to predict the encoded kbid.\n",
    "Decodes the prediction back to the original KBID value.\n",
    "Ensure Consistency\n",
    "Verify that cat_item and short_description follow the same preprocessing steps used during training.\n",
    "Make sure the column alignment (input_features.reindex()) matches the training feature structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacee6fa-82c7-463e-986d-8065f7f9cf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd35756-372d-4b4f-ada6-97363f2226a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b71dc6-17a0-4b6e-98ce-b8001ee73f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d3e2a-c850-4656-8db9-4b2e972e8d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ed715-4fd5-4169-928c-97bb8ce6e4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c0e53-5f29-4434-8db7-3fefd6ecb510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
