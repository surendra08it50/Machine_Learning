{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ef6a5-653a-45c0-aa9c-03de4b1044ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f014e0ef-80b7-4a4d-8f3b-b4cd62f0187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f8147-5320-4276-9070-f450a7e22b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1d8058-c539-4ce0-8779-60ae9b95da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "file_path = 'sctask_data_dev_v13_utf8.csv'  # Update with your file path\n",
    "data = pd.read_csv(file_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1093dd15-630b-4321-b5b8-39228231c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'cat_item' as a numerical feature\n",
    "cat_item_encoder = LabelEncoder()\n",
    "data['cat_item_encoded'] = cat_item_encoder.fit_transform(data['cat_item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ca8ed8-3b12-42c9-b05e-0884b5032713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the short descriptions\n",
    "data['short_description_tokenized'] = data['short_description'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c01a76d-9521-4145-a3ae-d6f2a608a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                  Basic_Mainframe_Access       1.00      1.00      1.00        15\n",
      "                           Clean_Up_RACF       1.00      1.00      1.00        19\n",
      "                               GuestWifi       1.00      1.00      1.00       109\n",
      "      Kiteworks_Add_User_Existing_Folder       1.00      1.00      1.00        63\n",
      "      Kiteworks_Create_New_Secure_Folder       1.00      1.00      1.00        66\n",
      "          Kiteworks_Remove_Secure_Folder       1.00      1.00      1.00        46\n",
      "            Off_Boarding_Integrity_Check       1.00      1.00      1.00        15\n",
      "                                  Others       1.00      1.00      1.00      2654\n",
      "                 Remove_Mainframe_Access       1.00      1.00      1.00        23\n",
      "                computer_integrity_check       1.00      1.00      1.00        25\n",
      "                   corporate_access_d365       1.00      1.00      1.00        52\n",
      "                      cross_boarding_imc       1.00      1.00      1.00        28\n",
      "                           intune_device       1.00      1.00      1.00        19\n",
      "          kiteworks_modify_secure_folder       1.00      1.00      1.00        16\n",
      "kiteworks_remove_user_from_secure_folder       1.00      1.00      1.00        36\n",
      "              review_offboarding_request       1.00      1.00      1.00        75\n",
      "\n",
      "                                accuracy                           1.00      3261\n",
      "                               macro avg       1.00      1.00      1.00      3261\n",
      "                            weighted avg       1.00      1.00      1.00      3261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train a Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=data['short_description_tokenized'], \n",
    "    vector_size=100,  # Size of word vectors\n",
    "    window=5,         # Context window size\n",
    "    min_count=1,      # Minimum word frequency\n",
    "    workers=4,        # Number of CPU threads\n",
    "    sg=1              # Skip-gram model\n",
    ")\n",
    "\n",
    "# Generate sentence embeddings by averaging word vectors\n",
    "def vectorize_sentence(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "data['short_description_vector'] = data['short_description_tokenized'].apply(\n",
    "    lambda tokens: vectorize_sentence(tokens, w2v_model, vector_size=100)\n",
    ")\n",
    "\n",
    "# Create the feature matrix\n",
    "X = pd.DataFrame(data['short_description_vector'].tolist())  # Expand vectors into separate columns\n",
    "X['cat_item_encoded'] = data['cat_item_encoded']\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "# Encode the target 'kbid'\n",
    "kbid_encoder = LabelEncoder()\n",
    "data['kbid_encoded'] = kbid_encoder.fit_transform(data['kbid'])\n",
    "y = data['kbid_encoded']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Fores\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=kbid_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334921f-45fa-48df-93a4-2697f4852723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2310cd-e66e-4faf-ab73-43304245cdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb6e63-9a13-47a3-9333-f483d15a38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d205e2-3357-4f01-b3eb-650d1b0c0c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06ee93-77d9-4ee9-abea-d59f87ddabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use spacy instead of .split() function as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99b2d7-4c92-413a-899f-8612b06e888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from gensim.models import Word2Vec\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Load SpaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Tokenize short descriptions using SpaCy\n",
    "# def tokenize_with_spacy(text):\n",
    "#     return [token.text.lower() for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "\n",
    "# data['short_description_tokenized'] = data['short_description'].apply(tokenize_with_spacy)\n",
    "\n",
    "# # Train Word2Vec as before\n",
    "# w2v_model = Word2Vec(\n",
    "#     sentences=data['short_description_tokenized'], \n",
    "#     vector_size=100, \n",
    "#     window=5, \n",
    "#     min_count=1, \n",
    "#     workers=4, \n",
    "#     sg=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bac4d-3c96-4e13-a154-5fa87e4301fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7cd29-e77c-490b-b55e-b1e03571f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc82fe-4ebd-49c0-8602-ab700203abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'sctask_data_dev_v13_utf8.csv'  # Update with your file path\n",
    "data = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Encode 'cat_item' as a numerical feature\n",
    "cat_item_encoder = LabelEncoder()\n",
    "data['cat_item_encoded'] = cat_item_encoder.fit_transform(data['cat_item'])\n",
    "\n",
    "# Load Hugging Face tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the short descriptions\n",
    "def tokenize_with_hf(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "data['short_description_tokenized'] = data['short_description'].apply(tokenize_with_hf)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=data['short_description_tokenized'], \n",
    "    vector_size=100,  # Size of word vectors\n",
    "    window=5,         # Context window size\n",
    "    min_count=1,      # Minimum word frequency\n",
    "    workers=4,        # Number of CPU threads\n",
    "    sg=1              # Skip-gram model\n",
    ")\n",
    "\n",
    "# Generate sentence embeddings by averaging word vectors\n",
    "def vectorize_sentence(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "data['short_description_vector'] = data['short_description_tokenized'].apply(\n",
    "    lambda tokens: vectorize_sentence(tokens, w2v_model, vector_size=100)\n",
    ")\n",
    "\n",
    "# Create the feature matrix\n",
    "X = pd.DataFrame(data['short_description_vector'].tolist())  # Expand vectors into separate columns\n",
    "X['cat_item_encoded'] = data['cat_item_encoded']\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "\n",
    "# Encode the target 'kbid'\n",
    "kbid_encoder = LabelEncoder()\n",
    "data['kbid_encoded'] = kbid_encoder.fit_transform(data['kbid'])\n",
    "y = data['kbid_encoded']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=kbid_encoder.classes_))\n",
    "\n",
    "# Save the encoders, model, and Word2Vec\n",
    "joblib.dump(model, 'kbid_predictor_model.pkl')\n",
    "joblib.dump(cat_item_encoder, 'cat_item_encoder.pkl')\n",
    "joblib.dump(kbid_encoder, 'kbid_encoder.pkl')\n",
    "w2v_model.save('word2vec_model.bin')\n",
    "\n",
    "# Usage example\n",
    "def predict_kbid(cat_item, short_description):\n",
    "    # Encode the categorical input\n",
    "    cat_item_encoded = cat_item_encoder.transform([cat_item])[0]\n",
    "    \n",
    "    # Tokenize and vectorize the short description\n",
    "    tokens = tokenizer.tokenize(short_description)\n",
    "    short_desc_vectorized = vectorize_sentence(tokens, w2v_model, vector_size=100)\n",
    "    \n",
    "    # Combine features\n",
    "    input_features = pd.DataFrame([short_desc_vectorized.tolist()])\n",
    "    input_features['cat_item_encoded'] = cat_item_encoded\n",
    "    \n",
    "    # Align columns with training data\n",
    "    input_features = input_features.reindex(columns=X.columns, fill_value=0)\n",
    "    \n",
    "    # Predict and decode the KBID\n",
    "    kbid_encoded = model.predict(input_features)[0]\n",
    "    return kbid_encoder.inverse_transform([kbid_encoded])[0]\n",
    "\n",
    "# Example prediction\n",
    "example_prediction = predict_kbid(\"Asset Management\", \"Dax Studio - Power BI Software Tool Integration\")\n",
    "print(\"Predicted KBID:\", example_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca20555-cd19-473d-be8b-97e0fb05ba68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
